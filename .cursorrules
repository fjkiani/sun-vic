# Instructions

During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./venv. Use it.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## LLM Integration Lessons
- Gemini Pro doesn't support `withStructuredOutput` like Claude/Anthropic - need different approach for structured output
- Keep existing Claude implementation while testing Gemini integration to ensure smooth transition
- Need to modify prompts to work with Gemini's response format
- Consider creating wrapper/adapter for consistent API across different LLM providers

## Current Task: Integrated Room Design & Shopping System

### Workflow Analysis (Interior Designer + Construction Perspective)

1. Design Phase (Replicate AI):
   - Takes user's room photo
   - Applies design style
   - Generates redesigned space
   - Acts like initial design mockup

2. Analysis Phase (Gemini Vision):
   - Like an interior designer analyzing the space
   - Identifies key elements:
     ```typescript
     interface RoomAnalysis {
         structural: {
             walls: string[];      // "exposed brick", "painted drywall"
             flooring: string[];   // "hardwood", "tile"
             windows: string[];    // "floor-to-ceiling", "bay window"
         };
         furniture: {
             type: string;         // "sectional sofa", "dining table"
             style: string[];      // "mid-century modern", "industrial"
             materials: string[];  // "leather", "reclaimed wood"
             dimensions: string;   // "approximate size/space needed"
         }[];
         decor: {
             lighting: string[];   // "pendant lights", "floor lamps"
             accessories: string[];// "wall art", "throw pillows"
             colors: string[];     // "navy blue", "brushed brass"
         };
     }
     ```

3. Shopping Phase (Google Shopping API):
   ```typescript
   interface ProductSearch {
       endpoint: string;  // "search" or "search-v2"
       parameters: {
           keyword: string;      // Generated from analysis
           price_min?: number;
           price_max?: number;
           gl?: string;         // Country code
           filter_by?: string[];// "Free shipping", "On sale"
       };
   }
   ```

### Integration Flow

1. Room Generation:
```javascript
// 1. Generate new design
const newDesign = await replicate.run(model, {
    image: originalRoom,
    prompt: designPrompt
});

// 2. Analyze generated design
const analysis = await gemini.analyzeImage({
    image: newDesign,
    roomType,
    designStyle
});

// 3. Generate shopping recommendations
const recommendations = await Promise.all(
    analysis.furniture.map(item => 
        googleShopping.search({
            keyword: `${item.style[0]} ${item.type} ${item.materials[0]}`,
            gl: 'us',
            price_range: userPriceRange
        })
    )
);
```

2. Response Structure:
```typescript
interface DesignResponse {
    design: {
        original: string;    // Original room URL
        generated: string;   // AI generated room URL
    };
    analysis: {
        structural: any[];   // Construction considerations
        furniture: any[];    // Furniture recommendations
        decor: any[];       // Decor elements
    };
    shopping: {
        furniture: Product[];
        decor: Product[];
        materials: Product[];
    };
}
```

### Implementation Phases

#### Phase 1: Core Integration [Current Focus]
[X] Setup Replicate room generation
[ ] Implement Gemini Vision analysis
[ ] Configure Google Shopping API
[ ] Create unified response structure

#### Phase 2: Enhanced Analysis
[ ] Structural element detection
[ ] Material recommendations
[ ] Size/dimension estimation
[ ] Style consistency validation

#### Phase 3: Shopping Integration
[ ] Product category mapping
[ ] Price range optimization
[ ] Availability checking
[ ] Regional store filtering

#### Phase 4: UI/UX Development
[ ] Split-view comparison
[ ] Product overlay on image
[ ] Material/finish selector
[ ] Budget calculator

### Technical Considerations

1. API Coordination:
   - Sequential processing
   - Error handling between services
   - Response caching
   - Rate limit management

2. Data Structure:
   - Consistent format across services
   - Flexible enough for different room types
   - Structured for easy UI rendering

3. Performance:
   - Parallel product searches
   - Progressive loading
   - Image optimization
   - Cache frequently requested items

### Next Steps
1. [ ] Create unified API handler
2. [ ] Implement analysis pipeline
3. [ ] Setup shopping integration
4. [ ] Design response structure

Would you like me to start implementing any specific component?

# Scratchpad

## Current Task Review
Implementing Guest/Trial Mode for AI Room Design Application

## Current State Analysis
1. Core Files Reviewed:
   - app/dashboard/create-new/page.jsx: Main image generation logic
   - app/dashboard/_components/Header.jsx: Navigation and user interface
   - app/page.js: Landing page
   - app/api/verify-user/route.jsx: User verification
   - config/schema.js: Database schema

2. Current Authentication Flow:
   - Uses Clerk for authentication
   - All routes require authentication
   - Credits system tied to user accounts
   - Image generation requires user email

3. Key Dependencies:
   - Firebase for image storage
   - Replicate for AI generation
   - Clerk for auth
   - Drizzle ORM for database

## Implementation Challenges & Considerations
1. Data Storage:
   - Need separate storage for guest generations
   - Must implement expiration for guest data
   - Consider storage limits for guests

2. Security:
   - Prevent abuse of guest system
   - Rate limiting for guest requests
   - Secure session management

3. User Experience:
   - Clear trial limitations
   - Smooth upgrade path
   - Persistent guest session across pages

## Revised Implementation Plan

### Phase 1: Database & Schema [Priority]
1. [ ] Add GuestGeneratedImage table
   - Session tracking
   - Expiration handling
   - Usage limits

2. [ ] Update existing routes
   - Handle guest sessions
   - Implement usage checks
   - Add error handling

### Phase 2: Guest Session Management
1. [ ] Create guest session system
   - Local storage based
   - 24hr expiration
   - Usage tracking

2. [ ] Update middleware
   - Allow guest routes
   - Implement rate limiting
   - Handle auth bypassing

### Phase 3: UI Components
1. [ ] Update Header
   - Add trial mode indicator
   - Show remaining generations
   - Add upgrade prompts

2. [ ] Modify Create New page
   - Handle guest flow
   - Show limitations
   - Add conversion CTAs

### Phase 4: API Routes
1. [ ] Update redesign-room
   - Handle guest requests
   - Implement limits
   - Store guest results

2. [ ] Modify verify-user
   - Add guest verification
   - Track usage
   - Handle upgrades

## Technical Notes
1. Session Management:
   - Use localStorage for guest session
   - Implement session cleanup
   - Track usage count

2. Database Considerations:
   - Regular cleanup of expired guest data
   - Index on sessionId and expiresAt
   - Optimize queries for guest checks

3. Security Measures:
   - Rate limit by IP
   - Validate guest tokens
   - Prevent session manipulation

## Progress Tracking
[X] Initial Analysis
  - Reviewed core files
  - Identified key changes
  - Listed dependencies

[ ] Schema Updates
  - Design guest table
  - Plan migrations
  - Update queries

[ ] Session Implementation
  - Design session structure
  - Plan storage strategy
  - Define limitations

## Lessons Learned
- Keep guest functionality separate from user functionality
- Implement proper cleanup for guest data
- Consider rate limiting from the start
- Design clear upgrade paths

Would you like me to proceed with implementing any specific phase of this revised plan?

## Current Priority: Debug Gemini Analysis Integration

### Latest Error Analysis (2024-02-17)
Problem: Analysis still failing with 500 status despite successful image access

#### Current Flow Verification
1. Image Generation ✅
   - Replicate generates image successfully
   - Firebase stores image successfully
   - URL is accessible (confirmed via curl)

2. Analysis Attempt ❌
   - Request reaches analyze-room endpoint
   - 500 error returned
   - No detailed error logs visible

#### Debug Hypothesis
1. Base64 Encoding Issues
   ```javascript
   // Current
   const base64Image = Buffer.from(imageResponse.data).toString('base64');
   
   // Potential fixes to try:
   // 1. Add data URI prefix
   const base64WithPrefix = `data:image/jpeg;base64,${base64Image}`;
   
   // 2. Use FileReader API
   const blob = new Blob([imageResponse.data], { type: 'image/jpeg' });
   const reader = new FileReader();
   reader.readAsDataURL(blob);
   ```

2. Request Format Issues
   ```javascript
   // Current
   {
       inlineData: {
           mimeType: 'image/jpeg',
           data: base64Image
       }
   }
   
   // Alternative to try
   {
       parts: [{
           text: prompt
       }, {
           inlineData: {
               mimeType: 'image/jpeg',
               data: base64Image
           }
       }]
   }
   ```

#### Next Debug Steps [Priority Order]
1. [ ] Add verbose error logging
   ```javascript
   // In analyze-room/route.jsx
   console.error('Full error details:', {
       error: error.message,
       stack: error.stack,
       response: error.response?.data,
       status: error.response?.status,
       headers: error.response?.headers
   });
   ```

2. [ ] Test with minimal prompt
   ```javascript
   // Simplify prompt to rule out parsing issues
   const simplePrompt = "Describe what you see in this image";
   ```

3. [ ] Verify Gemini API limits
   - Check base64 size limits
   - Verify API quota
   - Test rate limiting

4. [ ] Add request validation
   ```javascript
   // Before sending to Gemini
   console.log('Request payload:', {
       promptLength: prompt.length,
       base64Length: base64Image.length,
       mimeType: 'image/jpeg'
   });
   ```

#### Questions to Investigate
1. Is the error from Gemini API or our processing?
2. Are we hitting any size/format limitations?
3. Is the prompt format causing issues?
4. Are we handling the response stream correctly?

#### Action Items
1. [ ] Implement detailed error logging
2. [ ] Test with simplified prompt
3. [ ] Add request validation
4. [ ] Test with different image sizes
5. [ ] Verify Gemini API quotas

Would you like me to implement any of these debug steps?

[Previous notes remain below...]